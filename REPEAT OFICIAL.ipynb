{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd2a711c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#IMPORTING LIBRARIE\n",
    "import pandas as pd\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.window import Window\n",
    "from pyspark import SparkConf\n",
    "from pyspark.sql.functions import to_timestamp\n",
    "\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51102677",
   "metadata": {},
   "source": [
    "## Reading Data from Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "32c64083",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/01/03 16:49:28 WARN Utils: Your hostname, BDS-2023 resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "24/01/03 16:49:28 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/01/03 16:49:30 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "spark_conf = SparkConf().setMaster(\"local[*]\").setAppName(\"Tweets_Hadoop\")\n",
    "\n",
    "spark = SparkSession.builder.config(conf=spark_conf).config('spark.sql.session.timeZone', 'UTC').getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "sc.setLogLevel('ERROR')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3fac4ebc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_spark = spark.read.parquet(\"hdfs://localhost:9000/CA4/TWEETS_NEW.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c41acbed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Index: long (nullable = true)\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Date/Time: string (nullable = true)\n",
      " |-- Info: string (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- Tweet: string (nullable = true)\n",
      " |-- Tokenized_Tweet: array (nullable = true)\n",
      " |    |-- element: string (containsNull = true)\n",
      " |-- __index_level_0__: long (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "871aa8d7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+-----------------+--------------------+--------------------+--------+--------------------+--------------------+-----------------+\n",
      "|summary|            Index|                  ID|           Date/Time|    Info|                User|               Tweet|__index_level_0__|\n",
      "+-------+-----------------+--------------------+--------------------+--------+--------------------+--------------------+-----------------+\n",
      "|  count|            83941|               83941|               83941|   83941|               83941|               83941|            83941|\n",
      "|   mean|876774.3877604508|1.9935392360531683E9|                null|    null| 8.126396164285715E7|                null|876774.3877604508|\n",
      "| stddev|455942.6130676904|1.8773316488771933E8|                null|    null|2.5422699526890153E8|                null|455942.6130676904|\n",
      "|    min|               18|          1467813579|Fri Apr 17 20:30:...|NO_QUERY|            007peter|            Miss ...|               18|\n",
      "|    25%|           492103|          1956906359|                null|    null|            101006.0|                null|           492103|\n",
      "|    50%|           926391|          2000755670|                null|    null|            311081.0|                null|           926391|\n",
      "|    75%|          1267318|          2071645176|                null|    null|           1791995.0|                null|          1267318|\n",
      "|    max|          1599960|          2329204554|Wed May 27 07:27:...|NO_QUERY|                zzap|ï¿½anisalovesu me...|          1599960|\n",
      "+-------+-----------------+--------------------+--------------------+--------+--------------------+--------------------+-----------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_spark.summary().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4ec61f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "conf = SparkConf().set(\"spark.sql.legacy.timeParserPolicy\", \"LEGACY\")\n",
    "spark = spark.builder.config(conf=conf).getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2fdb30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_spark = tweets_spark.withColumn(\"Date/Time\", to_timestamp(tweets_spark[\"Date/Time\"], \"EEE MMM dd HH:mm:ss zzz yyyy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "783b79b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_spark = tweets_spark.select(\"ID\", \"Date/Time\", \"User\", \"Tweet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b43c4850",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+--------------------+\n",
      "|        ID|          Date/Time|         User|               Tweet|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "|1467813579|2009-04-07 05:20:31|   starkissed|@LettyA ahh ive a...|\n",
      "|1467814438|2009-04-07 05:20:44|ChicagoCubbie|I hate when I hav...|\n",
      "|1467816149|2009-04-07 05:21:11|     Pbearfox|@julieebaby awe i...|\n",
      "|1467818603|2009-04-07 05:21:49|    kennypham|Sad, sad, sad. I ...|\n",
      "|1467822384|2009-04-07 05:22:47|  Lindsey0920|@oanhLove I hate ...|\n",
      "+----------+-------------------+-------------+--------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 5:=============================>                             (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_spark.show(truncate=True, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dc3af190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- ID: long (nullable = true)\n",
      " |-- Date/Time: timestamp (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- Tweet: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_spark.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b252b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year, month, count, format_string, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f121cecb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 9:=======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+-----+-----+----------+\n",
      "|year|month|count|percentage|\n",
      "+----+-----+-----+----------+\n",
      "|2009|4    |4943 |5.89%     |\n",
      "|2009|5    |30346|36.15%    |\n",
      "|2009|6    |48652|57.96%    |\n",
      "+----+-----+-----+----------+\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "\n",
    "df = tweets_spark.groupBy(year(\"Date/Time\").alias(\"year\"), month(\"Date/Time\").alias(\"month\")).count() \\\n",
    "                 .orderBy([\"year\", \"month\"])\n",
    "\n",
    "\n",
    "df = df.withColumn(\"percentage\", format_string(\"%.2f%%\", ((col(\"count\")/tweets_spark.count())*100)))\n",
    "\n",
    "\n",
    "df.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8ab830",
   "metadata": {},
   "source": [
    "# Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "305002a0",
   "metadata": {},
   "source": [
    "## Logistic Regression Classifier Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "39e263ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.classification import LogisticRegression\n",
    "from pyspark.ml.feature import HashingTF, Tokenizer, StopWordsRemover\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "91a20fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+---------+---------------+---------------------------------+\n",
      "|ItemID|Sentiment|SentimentSource|SentimentText                    |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "|1038  |1        |Sentiment140   |that film is fantastic #brilliant|\n",
      "|1804  |1        |Sentiment140   |this music is really bad #myband |\n",
      "|1693  |0        |Sentiment140   |winter is terrible #thumbs-down  |\n",
      "+------+---------+---------------+---------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#read csv file into dataFrame with automatically inferred schema\n",
    "tweets_csv = spark.read.csv(\"hdfs://localhost:9000/CA4/tweets.csv\", inferSchema=True, header=True)\n",
    "tweets_csv.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d8766d16",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------------------------+-----+\n",
      "|SentimentText                    |label|\n",
      "+---------------------------------+-----+\n",
      "|that film is fantastic #brilliant|1    |\n",
      "|this music is really bad #myband |1    |\n",
      "|winter is terrible #thumbs-down  |0    |\n",
      "|this game is awful #nightmare    |0    |\n",
      "|I love jam #loveit               |1    |\n",
      "+---------------------------------+-----+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#select only \"SentimentText\" and \"Sentiment\" column, \n",
    "#and cast \"Sentiment\" column data into integer\n",
    "data = tweets_csv.select(\"SentimentText\", col(\"Sentiment\").cast(\"Int\").alias(\"label\"))\n",
    "data.show(truncate = False,n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "83a7bf17",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 19:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data rows: 1339 ; Testing data rows: 593\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "#divide data, 70% for training, 30% for testing\n",
    "dividedData = data.randomSplit([0.7, 0.3]) \n",
    "trainingData = dividedData[0] #index 0 = data training\n",
    "testingData = dividedData[1] #index 1 = data testing\n",
    "train_rows = trainingData.count()\n",
    "test_rows = testingData.count()\n",
    "print (\"Training data rows:\", train_rows, \"; Testing data rows:\", test_rows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "da335a37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 22:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+----------------------------------------+\n",
      "|SentimentText                     |label|SentimentWords                          |\n",
      "+----------------------------------+-----+----------------------------------------+\n",
      "|I adore cheese #brilliant         |1    |[i, adore, cheese, #brilliant]          |\n",
      "|I adore cheese #favorite          |1    |[i, adore, cheese, #favorite]           |\n",
      "|I adore cheese #loveit            |1    |[i, adore, cheese, #loveit]             |\n",
      "|I adore cheese #thumbs-up         |1    |[i, adore, cheese, #thumbs-up]          |\n",
      "|I adore classical music #brilliant|1    |[i, adore, classical, music, #brilliant]|\n",
      "+----------------------------------+-----+----------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"SentimentText\", outputCol=\"SentimentWords\")\n",
    "tokenizedTrain = tokenizer.transform(trainingData)\n",
    "tokenizedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a5432c96",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 23:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
      "|SentimentText                     |label|SentimentWords                          |MeaningfulWords                      |\n",
      "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
      "|I adore cheese #brilliant         |1    |[i, adore, cheese, #brilliant]          |[adore, cheese, #brilliant]          |\n",
      "|I adore cheese #favorite          |1    |[i, adore, cheese, #favorite]           |[adore, cheese, #favorite]           |\n",
      "|I adore cheese #loveit            |1    |[i, adore, cheese, #loveit]             |[adore, cheese, #loveit]             |\n",
      "|I adore cheese #thumbs-up         |1    |[i, adore, cheese, #thumbs-up]          |[adore, cheese, #thumbs-up]          |\n",
      "|I adore classical music #brilliant|1    |[i, adore, classical, music, #brilliant]|[adore, classical, music, #brilliant]|\n",
      "+----------------------------------+-----+----------------------------------------+-------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemovedTrain = swr.transform(tokenizedTrain)\n",
    "SwRemovedTrain.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b709be60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+-------------------------------------------+\n",
      "|label|MeaningfulWords            |features                                   |\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "|1    |[adore, cheese, #brilliant]|(262144,[1689,45361,100089],[1.0,1.0,1.0]) |\n",
      "|1    |[adore, cheese, #favorite] |(262144,[1689,100089,108624],[1.0,1.0,1.0])|\n",
      "|1    |[adore, cheese, #loveit]   |(262144,[1689,100089,254974],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+-------------------------------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericTrainData = hashTF.transform(SwRemovedTrain).select(\n",
    "    'label', 'MeaningfulWords', 'features')\n",
    "numericTrainData.show(truncate=False, n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee6a2fd9",
   "metadata": {},
   "source": [
    "## Training the model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "cc1e5392",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training is done!\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression(labelCol=\"label\", featuresCol=\"features\", \n",
    "                        maxIter=10, regParam=0.01)\n",
    "model = lr.fit(numericTrainData)\n",
    "print (\"Training is done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3992c59e",
   "metadata": {},
   "source": [
    "## Testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "a9da42bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 37:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------------------------+------------------------------------------+\n",
      "|Label|MeaningfulWords            |features                                  |\n",
      "+-----+---------------------------+------------------------------------------+\n",
      "|1    |[adore, cheese, #bestever] |(262144,[1689,91011,100089],[1.0,1.0,1.0])|\n",
      "|1    |[adore, cheese, #toptastic]|(262144,[1689,42010,100089],[1.0,1.0,1.0])|\n",
      "+-----+---------------------------+------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tokenizedTest = tokenizer.transform(testingData)\n",
    "SwRemovedTest = swr.transform(tokenizedTest)\n",
    "numericTest = hashTF.transform(SwRemovedTest).select(\n",
    "    'Label', 'MeaningfulWords', 'features')\n",
    "numericTest.show(truncate=False, n=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d148e987",
   "metadata": {},
   "source": [
    "## Predicting testing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "996f50c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------------------------------+----------+-----+\n",
      "|MeaningfulWords                     |prediction|Label|\n",
      "+------------------------------------+----------+-----+\n",
      "|[adore, cheese, #bestever]          |1.0       |1    |\n",
      "|[adore, cheese, #toptastic]         |1.0       |1    |\n",
      "|[adore, classical, music, #bestever]|1.0       |1    |\n",
      "|[adore, classical, music, #loveit]  |1.0       |1    |\n",
      "+------------------------------------+----------+-----+\n",
      "only showing top 4 rows\n",
      "\n",
      "correct prediction: 584 , total data: 593 , accuracy: 0.984822934232715\n"
     ]
    }
   ],
   "source": [
    "prediction = model.transform(numericTest)\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\", \"Label\")\n",
    "predictionFinal.show(n=4, truncate = False)\n",
    "correctPrediction = predictionFinal.filter(\n",
    "    predictionFinal['prediction'] == predictionFinal['Label']).count()\n",
    "totalData = predictionFinal.count()\n",
    "print(\"correct prediction:\", correctPrediction, \", total data:\", totalData, \n",
    "      \", accuracy:\", correctPrediction/totalData)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c67e4bc",
   "metadata": {},
   "source": [
    "## Tweets Dataset - cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3560f9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import regexp_replace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "eb3ac495",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Definição de expressões regulares\n",
    "at_regex = r\"@\\w+\"  # Remove usernames\n",
    "link_regex = r\"http\\S+\"  # Remove links\n",
    "rt_regex = r'\\bRT\\b'  # Remove 'RT'\n",
    "ss_regex = r'[^\\w\\s]'  # Remove Special strings\n",
    "ds_regex = r'\\s+'  # Remove spaces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64a5b4bd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+-------------+-----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n",
      "|ID        |Date/Time          |User         |Tweet                                                                                    |clean_tweet                                                                       |\n",
      "+----------+-------------------+-------------+-----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n",
      "|1467813579|2009-04-07 05:20:31|starkissed   |@LettyA ahh ive always wanted to see rent  love the soundtrack!!                         | ahh ive always wanted to see rent love the soundtrack                            |\n",
      "|1467814438|2009-04-07 05:20:44|ChicagoCubbie|I hate when I have to call and wake people up                                            |I hate when I have to call and wake people up                                     |\n",
      "|1467816149|2009-04-07 05:21:11|Pbearfox     |@julieebaby awe i love you too!!!! 1 am here  i miss you                                 | awe i love you too 1 am here i miss you                                          |\n",
      "|1467818603|2009-04-07 05:21:49|kennypham    |Sad, sad, sad. I don't know why but I hate this feeling  I wanna sleep and I still can't!|Sad sad sad I dont know why but I hate this feeling I wanna sleep and I still cant|\n",
      "|1467822384|2009-04-07 05:22:47|Lindsey0920  |@oanhLove I hate when that happens...                                                    | I hate when that happens                                                         |\n",
      "+----------+-------------------+-------------+-----------------------------------------------------------------------------------------+----------------------------------------------------------------------------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_spark = tweets_spark.withColumn(\"clean_tweet\", regexp_replace(\"Tweet\", at_regex, \"\"))\n",
    "tweets_spark = tweets_spark.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", link_regex, \"\"))\n",
    "tweets_spark = tweets_spark.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", rt_regex, \"\"))\n",
    "tweets_spark = tweets_spark.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", ss_regex, \"\"))\n",
    "tweets_spark = tweets_spark.withColumn(\"clean_tweet\", regexp_replace(\"clean_tweet\", ds_regex, \" \"))\n",
    "\n",
    "# Exibição dos resultados\n",
    "tweets_spark.show(truncate=False, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "d84a3692",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+\n",
      "|     MeaningfulWords|            features|\n",
      "+--------------------+--------------------+\n",
      "|[, ahh, ive, alwa...|(262144,[8538,524...|\n",
      "|[hate, call, wake...|(262144,[72709,10...|\n",
      "|[, awe, love, 1, ...|(262144,[92651,13...|\n",
      "+--------------------+--------------------+\n",
      "only showing top 3 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = Tokenizer(inputCol=\"clean_tweet\", outputCol=\"words\")\n",
    "tokenizedData = tokenizer.transform(tweets_spark)\n",
    "\n",
    "swr = StopWordsRemover(inputCol=tokenizer.getOutputCol(), \n",
    "                       outputCol=\"MeaningfulWords\")\n",
    "SwRemoved = swr.transform(tokenizedData)\n",
    "\n",
    "hashTF = HashingTF(inputCol=swr.getOutputCol(), outputCol=\"features\")\n",
    "numericData = hashTF.transform(SwRemoved).select('MeaningfulWords', 'features')\n",
    "\n",
    "\n",
    "numericData.show(n=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd062d6",
   "metadata": {},
   "source": [
    "## Predicting Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "2d359326",
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction = model.transform(numericData)\n",
    "\n",
    "predictionFinal = prediction.select(\n",
    "    \"MeaningfulWords\", \"prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "75cab8ee",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------------------------------------------------------------------------------------------+----------+\n",
      "|MeaningfulWords                                                                                              |prediction|\n",
      "+-------------------------------------------------------------------------------------------------------------+----------+\n",
      "|[, ahh, ive, always, wanted, see, rent, love, soundtrack]                                                    |1.0       |\n",
      "|[hate, call, wake, people]                                                                                   |0.0       |\n",
      "|[, awe, love, 1, miss]                                                                                       |1.0       |\n",
      "|[sad, sad, sad, dont, know, hate, feeling, wanna, sleep, still, cant]                                        |0.0       |\n",
      "|[, hate, happens]                                                                                            |0.0       |\n",
      "|[really, hate, people, diss, bands, trace, clearly, ugly]                                                    |0.0       |\n",
      "|[sleep, soon, hate, saying, bye, see, tomorrow, night]                                                       |0.0       |\n",
      "|[, damn, grind, inspirational, saddening, time, dont, want, stop, cuz, like, u, much, love]                  |1.0       |\n",
      "|[late, night, snack, glass, oj, bc, im, quotdown, sicknessquot, back, sleepugh, hate, getting, sick]         |0.0       |\n",
      "|[im, missing, babe, long, alive, im, happy, yawwwnn, im, tired, love, imma, try, sleep, hopefully, headstart]|1.0       |\n",
      "+-------------------------------------------------------------------------------------------------------------+----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "[Stage 50:=============================>                            (1 + 1) / 2]\r",
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "predictionFinal.show(truncate = False, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "17b5edda",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "data": {
      "text/plain": [
       "83941"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictionFinal.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "25638f03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "|          Date/Time|           User|               Tweet|         clean_tweet|prediction|\n",
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "|2009-04-07 05:20:31|     starkissed|@LettyA ahh ive a...| ahh ive always w...|       1.0|\n",
      "|2009-04-07 05:20:44|  ChicagoCubbie|I hate when I hav...|I hate when I hav...|       0.0|\n",
      "|2009-04-07 05:21:11|       Pbearfox|@julieebaby awe i...| awe i love you t...|       1.0|\n",
      "|2009-04-07 05:21:49|      kennypham|Sad, sad, sad. I ...|Sad sad sad I don...|       0.0|\n",
      "|2009-04-07 05:22:47|    Lindsey0920|@oanhLove I hate ...| I hate when that...|       0.0|\n",
      "|2009-04-07 05:23:43|   BrookeAmanda|i really hate how...|i really hate how...|       0.0|\n",
      "|2009-04-07 05:25:52|     thelazyboy|sleep soon... i j...|sleep soon i just...|       0.0|\n",
      "|2009-04-07 05:27:08|         eyezup|@mercedesashley D...| Damn The grind i...|       1.0|\n",
      "|2009-04-07 05:27:34|    weefranniev|Late night snack,...|Late night snack ...|       0.0|\n",
      "|2009-04-07 05:28:04|        bnr0201|I'm missing you b...|Im missing you ba...|       1.0|\n",
      "|2009-04-07 05:32:28|     CaddyStage|@robluketic  love...| love the french ...|       1.0|\n",
      "|2009-04-07 05:32:38|       Jana1976|@JonathanRKnight ...| I hate the limit...|       0.0|\n",
      "|2009-04-07 05:33:16|stephaniekmusic|i hate to see the...|i hate to see the...|       0.0|\n",
      "|2009-04-07 05:37:04| latoyanlegania|Is still nursing ...|Is still nursing ...|       0.0|\n",
      "|2009-04-07 05:37:46|    flowerlilly|@DonnieWahlberg o...| ooh Im excited a...|       1.0|\n",
      "|2009-04-07 05:40:18|   ChrisJNewman|Think I'm going t...|Think Im going to...|       0.0|\n",
      "|2009-04-07 05:40:20|       stefanip|@riancurtis  i'm ...| im here friend a...|       1.0|\n",
      "|2009-04-07 05:41:09|    ARExistence|@austinhill I wis...| I wish I was Sol...|       1.0|\n",
      "|2009-04-07 05:43:26|        Mowgli3|http://twitpic.co...|    I love you Buck |       1.0|\n",
      "|2009-04-07 05:45:33|   scrappysgirl|is in love with s...|is in love with s...|       1.0|\n",
      "+-------------------+---------------+--------------------+--------------------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Create a column with id following the data's order \n",
    "tweets_spark = tweets_spark.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "predictionFinal = predictionFinal.withColumn(\"row_id\", monotonically_increasing_id())\n",
    "\n",
    "# join by \"row_id\"\n",
    "tweets_pred = tweets_spark.select('row_id','Date/Time','User', 'Tweet', 'clean_tweet') \\\n",
    "                    .join(predictionFinal.select('row_id', 'prediction'), \"row_id\", \"inner\")\n",
    "                \n",
    "\n",
    "# drop column \n",
    "tweets_pred = tweets_pred.drop(\"row_id\")\n",
    "\n",
    "tweets_pred.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ddf7a95",
   "metadata": {},
   "source": [
    "# Textblod and Varder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0365245b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from textblob import TextBlob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "61dca140",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     /home/hduser/nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('vader_lexicon')\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from textblob import TextBlob\n",
    "from pyspark.sql.functions import udf, col\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "21deaa85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "from pyspark.sql.types import FloatType\n",
    "\n",
    "# Função de análise de sentimento com TextBlob\n",
    "@udf(FloatType())\n",
    "def sentiment(tweet):\n",
    "    return TextBlob(tweet).sentiment.polarity\n",
    "\n",
    "# Função de análise de sentimento com VADER\n",
    "@udf(FloatType())\n",
    "def sentiment_vader(tweet):\n",
    "    sid = SentimentIntensityAnalyzer()\n",
    "    return sid.polarity_scores(tweet)['compound']\n",
    "\n",
    "# Aplicar diretamente no DataFrame\n",
    "tweets_pred = tweets_pred.withColumn(\"textblob\", sentiment(tweets_pred[\"clean_tweet\"])) \\\n",
    "                         .withColumn(\"vader\", sentiment_vader(tweets_pred[\"clean_tweet\"]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "de21f358",
   "metadata": {},
   "outputs": [],
   "source": [
    "tweets_pred = tweets_pred.withColumn(\"score\", ((col(\"prediction\") + (col(\"textblob\")*1.5) + (col(\"vader\")*1.5)) / 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7f887f2a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 59:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------+-----------+-------+--------------------+\n",
      "|         clean_tweet|prediction|   textblob|  vader|               score|\n",
      "+--------------------+----------+-----------+-------+--------------------+\n",
      "| ahh ive always w...|       1.0|        0.5| 0.6369|  0.6763375028967857|\n",
      "|I hate when I hav...|       0.0|       -0.8|-0.5719| -0.5144625082612038|\n",
      "| awe i love you t...|       1.0|        0.5| 0.5574|  0.6465249955654144|\n",
      "|Sad sad sad I don...|       0.0|     -0.575|-0.8505| -0.5345624908804893|\n",
      "| I hate when that...|       0.0|       -0.8|-0.5719| -0.5144625082612038|\n",
      "|i really hate how...|       0.0|     -0.225|  0.105|-0.04499999899417162|\n",
      "|sleep soon i just...|       0.0|       -0.8|-0.5719| -0.5144625082612038|\n",
      "| Damn The grind i...|       1.0| 0.33333334| 0.3975|  0.5240625068545341|\n",
      "|Late night snack ...|       0.0|-0.45357144|-0.7906|-0.46656429022550583|\n",
      "|Im missing you ba...|       1.0|      0.125| 0.9366|  0.6481000110507011|\n",
      "+--------------------+----------+-----------+-------+--------------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "tweets_pred.select(\"clean_tweet\", \"prediction\", \"textblob\", \"vader\", \"score\").show(n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "06cc5b55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Date/Time: timestamp (nullable = true)\n",
      " |-- User: string (nullable = true)\n",
      " |-- Tweet: string (nullable = true)\n",
      " |-- clean_tweet: string (nullable = true)\n",
      " |-- prediction: double (nullable = false)\n",
      " |-- textblob: float (nullable = true)\n",
      " |-- vader: float (nullable = true)\n",
      " |-- score: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tweets_pred.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ef4c6f",
   "metadata": {},
   "source": [
    "## Saving on Hadoop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "f8a743d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from hdfs import InsecureClient\n",
    "from hdfs.util import HdfsError"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "5a8f63db",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Colocando no Hadoop.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 61:======================================>                   (2 + 1) / 3]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Salvo em hdfs://localhost:9000/CA4/sentiment particionado por Date/Time\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import year, month, dayofmonth\n",
    "from hdfs import InsecureClient\n",
    "from hdfs.util import HdfsError\n",
    "\n",
    "def spark_hadoop(df, folder, partitionBy=None, spark=None):\n",
    "    hdfs_base_path = \"hdfs://localhost:9000\"\n",
    "    hdfs_folder_path = f\"{hdfs_base_path}/CA4/{folder}\"\n",
    "\n",
    "    client = InsecureClient('http://localhost:9870', user='hduser')\n",
    "\n",
    "    try:\n",
    "       \n",
    "        client.content(hdfs_folder_path)\n",
    "\n",
    "        print('Os arquivos já estão no Hadoop. Lendo os arquivos.')\n",
    "        df = spark.read.parquet(hdfs_folder_path)\n",
    "    except HdfsError:\n",
    "        print('Colocando no Hadoop.')\n",
    "        if partitionBy:\n",
    "            \n",
    "            df = df.withColumn(\"year\", year(\"Date/Time\"))\n",
    "            df = df.withColumn(\"month\", month(\"Date/Time\"))\n",
    "            df = df.withColumn(\"day\", dayofmonth(\"Date/Time\"))\n",
    "            \n",
    "           \n",
    "            df.write.partitionBy(\"year\", \"month\", \"day\").parquet(hdfs_folder_path)\n",
    "            print(f\"Salvo em {hdfs_folder_path} particionado por {partitionBy}\")\n",
    "        else:\n",
    "            df.write.parquet(hdfs_folder_path)\n",
    "            print(f\"Salvo em {hdfs_folder_path}\")\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "\n",
    "df_salvo = spark_hadoop(tweets_pred, folder=\"sentiment\", partitionBy=\"Date/Time\", spark=spark)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "32b7d7be",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 64:=============================>                            (1 + 1) / 2]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+-------------+--------------------+--------------------+----------+--------+-------+-------------------+----+-----+---+\n",
      "|          Date/Time|         User|               Tweet|         clean_tweet|prediction|textblob|  vader|              score|year|month|day|\n",
      "+-------------------+-------------+--------------------+--------------------+----------+--------+-------+-------------------+----+-----+---+\n",
      "|2009-04-07 05:20:31|   starkissed|@LettyA ahh ive a...| ahh ive always w...|       1.0|     0.5| 0.6369| 0.6763375028967857|2009|    4|  7|\n",
      "|2009-04-07 05:20:44|ChicagoCubbie|I hate when I hav...|I hate when I hav...|       0.0|    -0.8|-0.5719|-0.5144625082612038|2009|    4|  7|\n",
      "+-------------------+-------------+--------------------+--------------------+----------+--------+-------+-------------------+----+-----+---+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_salvo.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e76ae5d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
